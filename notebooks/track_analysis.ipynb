{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src/')\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Track analysis\n",
    "\n",
    "This notebook is meant for the analysis of vessel tracks with the purpose of finding suspicious behaviour. Suspicious behaviour could be:\n",
    "1. AIS gaps\n",
    "2. Loitering near infrastructure\n",
    "3. Slowing down near infrastructure\n",
    "4. Route deviation\n",
    "5. Sailing near critical infrastructure, such as naval bases\n",
    "6. Ship to ship transfers, to a certain degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import movingpandas as mpd\n",
    "\n",
    "from math import sin, cos, atan2, radians, degrees, sqrt, pi\n",
    "from pathlib import Path\n",
    "import string\n",
    "import random\n",
    "\n",
    "import datashader as ds\n",
    "import holoviews as hv\n",
    "from shapely.geometry import LineString\n",
    "from shapely import wkt\n",
    "from holoviews.operation.datashader import spread\n",
    "from holoviews.element import tiles\n",
    "from holoviews import opts\n",
    "import hvplot.pandas\n",
    "import folium\n",
    "from zipfile import ZipFile\n",
    "\n",
    "hv.extension('bokeh', 'matplotlib')\n",
    "\n",
    "R_EARTH = 6371000  # radius of earth in meters\n",
    "C_EARTH = 2 * R_EARTH * pi  # circumference\n",
    "BG_TILES = tiles.CartoLight()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set some constants\n",
    "\n",
    "PATH = Path.cwd().parent.joinpath('data')\n",
    "\n",
    "FIGSIZE = (600,400)\n",
    "SMSIZE = 300\n",
    "COLOR = 'darkblue'\n",
    "COLOR_HIGHLIGHT = 'red'\n",
    "COLOR_BASE = 'grey'\n",
    "\n",
    "# Create a bounding box for North Sea\n",
    "\n",
    "MIN_X = 0\n",
    "MIN_Y = 50\n",
    "MAX_X = 13\n",
    "MAX_Y = 58"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Prep data\n",
    "\n",
    "Import, clean and do some EDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let's create some useful functions (gracefully stolen from Anita Graser)\n",
    "\n",
    "def time_difference(row: pd.Series\n",
    "                    )-> pd.to_datetime:\n",
    "    t1 = row['prev_t']\n",
    "    t2 = row['time']\n",
    "\n",
    "    return (t2-t1).total_seconds()\n",
    "\n",
    "def speed_difference(row: pd.Series\n",
    "                     )-> float:\n",
    "    \n",
    "    return row['speed_m/s'] - row['prev_speed']\n",
    "\n",
    "def acceleration(row: pd.Series\n",
    "                 )-> float:\n",
    "    if row['diff_t_s'] == 0:\n",
    "        return None\n",
    "    \n",
    "    return row['diff_speed'] / row['diff_t_s']\n",
    "\n",
    "def spherical_distance(lon1: float, \n",
    "                       lat1: float, \n",
    "                       lon2: float, \n",
    "                       lat2: float\n",
    "                       )-> float:\n",
    "    \n",
    "    delta_lat = radians(lat2 - lat1)\n",
    "    delta_lon = radians(lon2 - lon1)\n",
    "    a = sin(delta_lat/2) * sin(delta_lat/2) + cos(radians(lat1)) * cos(radians(lat2)) * sin(delta_lon/2) * sin(delta_lon/2)\n",
    "    c = 2 * atan2(sqrt(a), sqrt(1 - a))\n",
    "    dist = R_EARTH * c\n",
    "    \n",
    "    return dist\n",
    "\n",
    "def distance_to_prev(row: pd.Series\n",
    "                     )-> float:\n",
    "    \n",
    "    return spherical_distance(row['prev_lon'], row['prev_lat'], row['longitude'], row['latitude'])\n",
    "    \n",
    "def distance_to_next(row: pd.Series\n",
    "                     )-> float:\n",
    "    \n",
    "    return spherical_distance(row['next_lon'], row['next_lat'], row['longitude'], row['latitude'])\n",
    "\n",
    "def direction(row: pd.Series\n",
    "              )-> float:\n",
    "    \n",
    "    lon1, lat1, lon2, lat2 = row['prev_lon'], row['prev_lat'], row['longitude'], row['latitude']\n",
    "    lat1 = radians(lat1)\n",
    "    lat2 = radians(lat2)\n",
    "    delta_lon = radians(lon2 - lon1)\n",
    "    x = sin(delta_lon) * cos(lat2)\n",
    "    y = cos(lat1) * sin(lat2) - (sin(lat1) * cos(lat2) * cos(delta_lon))\n",
    "    initial_bearing = atan2(x, y)\n",
    "    initial_bearing = degrees(initial_bearing)\n",
    "    compass_bearing = (initial_bearing + 360) % 360\n",
    "    \n",
    "    return compass_bearing\n",
    "\n",
    "def angular_difference(row: pd.Series\n",
    "                       )-> float:\n",
    "    \n",
    "    diff = abs(row['prev_dir'] - row['dir'])\n",
    "    if diff > 180:\n",
    "        diff = abs(diff - 360)\n",
    "\n",
    "    return diff \n",
    "\n",
    "def compute_segment_info(df: pd.DataFrame, \n",
    "                         identifier: str\n",
    "                         )-> pd.DataFrame:\n",
    "    \n",
    "    df = df.copy()\n",
    "    df['time'] = df.index\n",
    "    df = df.sort_values('time')\n",
    "    df['prev_t'] = df.groupby(identifier)['time'].shift()\n",
    "    df['diff_t_s'] = df.apply(time_difference, axis=1)\n",
    "    df['prev_lon'] = df.groupby(identifier)['longitude'].shift()\n",
    "    df['prev_lat'] = df.groupby(identifier)['latitude'].shift()\n",
    "    df['prev_x'] = df.groupby(identifier)['x'].shift()\n",
    "    df['prev_y'] = df.groupby(identifier)['y'].shift()\n",
    "    df['diff_x'] = df['x'] - df['prev_x']\n",
    "    df['diff_y'] = df['y'] - df['prev_y']\n",
    "    df['next_lon'] = df.groupby(identifier)['longitude'].shift(-1)\n",
    "    df['next_lat'] = df.groupby(identifier)['latitude'].shift(-1)\n",
    "    df['dist_prev_m'] = df.apply(distance_to_prev, axis=1)\n",
    "    df['dist_next_m'] = df.apply(distance_to_next, axis=1)\n",
    "    df['speed_m/s'] = df['dist_prev_m']/df['diff_t_s']\n",
    "    df['prev_speed'] = df.groupby(identifier)['speed_m/s'].shift()\n",
    "    df['diff_speed'] = df.apply(speed_difference, axis=1)\n",
    "    df['acceleration'] = df.apply(acceleration, axis=1)\n",
    "    df['dir'] = df.apply(direction, axis=1)\n",
    "    df['prev_dir'] = df.groupby(identifier)['dir'].shift()\n",
    "    df['diff_dir'] = df.apply(angular_difference, axis=1)\n",
    "    df.drop(columns=['prev_x', 'prev_y', 'next_lon', 'next_lat', 'prev_speed', 'prev_dir', 'time'], axis=1, inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def import_data(df: pd.DataFrame\n",
    "                )-> pd.DataFrame:\n",
    "    \n",
    "    #df = pd.read_csv(file)\n",
    "    #df.timestamp = pd.to_datetime(df.index)\n",
    "    df.sort_values(by='timestamp', inplace=True)\n",
    "    df['t'] = pd.to_datetime(df.timestamp)\n",
    "    df.drop('timestamp', inplace=True, axis=1)\n",
    "    df.set_index('t', drop=True, inplace=True)\n",
    "    #df['name'] = file.stem\n",
    "    #print(f'working on file: {file.stem}')\n",
    "\n",
    "    # Set x and y in meters\n",
    "    df = df.rename(columns={'lon': 'longitude', 'lat': 'latitude'})\n",
    "    df.loc[:, 'x'], df.loc[:, 'y'] = ds.utils.lnglat_to_meters(df.longitude, df.latitude)\n",
    "\n",
    "    df = compute_segment_info(df, 'vessel_name')\n",
    "\n",
    "    return df\n",
    "\n",
    "def create_gaps(df: pd.DataFrame, \n",
    "                identifier: str,\n",
    "                gap_min: int, \n",
    "                gap_max: int\n",
    "                )-> gpd.GeoDataFrame:\n",
    "\n",
    "    df['is_gap'] = ( (df['dist_prev_m']>gap_min) & (df['dist_prev_m']<gap_max ) | ( (df['dist_next_m']>gap_min) & (df['dist_next_m']<gap_max) ) )\n",
    "    df['id_by_gap'] = df.groupby(identifier)['is_gap'].transform(lambda x: x.ne(x.shift()).cumsum())\n",
    "    df = df[(df.prev_lat.notna()) | (df.prev_lon.notna())].copy()\n",
    "\n",
    "    df['geometry'] = df.apply(lambda row: LineString([(row.prev_lon, row.prev_lat), (row.longitude, row.latitude)]).wkt, axis=1)\n",
    "\n",
    "    gdf = df.copy()\n",
    "\n",
    "    gdf['geometry'] = gdf['geometry'].apply(wkt.loads)\n",
    "    gdf = gdf.set_geometry('geometry', crs=4326)\n",
    "\n",
    "    return gdf\n",
    "\n",
    "def plot_basic_scatter(df, color='darkblue', title='', width=FIGSIZE[0], height=FIGSIZE[1], size=2):\n",
    "    opts.defaults(opts.Overlay(active_tools=['wheel_zoom']))\n",
    "    pts = df.hvplot.scatter(x='x', y='y', datashade=True, cmap=[color, color], frame_width=width, frame_height=height, title=str(title))\n",
    "    return BG_TILES * spread(pts, px=size)\n",
    "\n",
    "def plot_point_density(df, title='', width=FIGSIZE[0], height=FIGSIZE[1]):\n",
    "    opts.defaults(opts.Overlay(active_tools=['wheel_zoom']))\n",
    "    pts = df.hvplot.scatter(x='x', y='y', title=str(title), datashade=True, frame_width=width, frame_height=height)\n",
    "    return BG_TILES * pts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data\n",
    "\n",
    "dfs = []\n",
    "for vessel in PATH.joinpath('voi', 'gfw_tracks', 'raw').rglob('*.zip'):\n",
    "    name = vessel.stem.split(' - ')[0]\n",
    "    zf = ZipFile(vessel)\n",
    "    df = pd.read_csv(zf.open('data.csv'))\n",
    "    df['vessel_name'] = name.lower()\n",
    "    dfs.append(df)\n",
    "\n",
    "vessels = pd.concat(dfs)\n",
    "\n",
    "len(vessels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write tracks to file\n",
    "df=import_data(vessels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.DataFrame(gdf.drop('geometry', axis=1))\n",
    "df.to_parquet(PATH.joinpath('voi', 'processed', 'vessel_collection.parquet'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform some exploratory data analysis\n",
    "\n",
    "We need to check for outliers, completeness and consistenc of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data\n",
    "\n",
    "gdf = gpd.read_parquet(PATH.joinpath('voi', 'processed', 'vessels_collection.parquet'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in gdf['name'].unique():\n",
    "    gdf_ = gdf[gdf['name'] == name].copy()\n",
    "    gdf_ = gdf_.cx[MIN_X:MIN_Y, MAX_X:MAX_Y]\n",
    "    gdf_['speed_kmh'] = round((gdf_.dist_prev_m / 1000) / (gdf_.diff_t_s / 3600))\n",
    "    gdf_ = gdf_[gdf_.speed_kmh < 20].copy()\n",
    "    gdf_.to_file(PATH.joinpath('gis', 'vessels.gpkg'), layer=name, driver='GPKG')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check spatial extent\n",
    "\n",
    "print(f'Spatial extent:\\nx_min={round(gdf.longitude.min(), 2)}, x_max={round(gdf.longitude.max(), 2)}\\ny_min={round(gdf.latitude.min(), 2)}, y_max={round(gdf.latitude.max(), 2)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check temporal extent\n",
    "\n",
    "print(f'Temporal extent:\\nmin_date={gdf.index.min()}\\nmax_date={gdf.index.max()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check distribution of records over time\n",
    "\n",
    "TIME_SAMPLE = '15min'\n",
    "\n",
    "gdf['name'].resample(TIME_SAMPLE).count()\\\n",
    "    .hvplot(title=f'Number of records per {TIME_SAMPLE}', width=FIGSIZE[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check histogram of directions to spot coordinate imprecisions\n",
    "\n",
    "gdf['dir'][gdf.dist_prev_m>0].hvplot.hist(bins=72, title='Histogram of directions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check histogram of speed (m/s)\n",
    "\n",
    "gdf['speed_m/s'][gdf['speed_m/s']>0].hvplot.hist(bins=72, title='Histogram of speed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check histogram of time difference\n",
    "\n",
    "gdf[gdf.diff_t_s < 20000].diff_t_s.hvplot.hist(bins=72, title='Histogram of time difference')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a very long tail of large time differences. This is probably caused by vessels being in the harbour for a long time (and AIS being turned off), being out of reach, or sailing without AIS. It's important to be mindful of these large temporal gaps, because they can also be an artefact of faulty equipment or poor reception and might be mistaken for deliberate AIS off switching."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Investigate gaps\n",
    "\n",
    "Some vessels have gaps in their AIS signals that we could investigate. These events could be important because turning off AIS could be indicative of the crew trying to hide their whereabouts. For this we should:\n",
    "1. Identify the points where a gap in AIS starts and end\n",
    "2. The average speed between those points and how that speed relate to the speed prior and after the gap event.\n",
    "3. Investigate the locations where these gaps are occuring\n",
    "4. Check if the AIS gap could be intentional or is an artefact of the location (crowded or out of reach of base stations)\n",
    "5. Find explanations for outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a subset\n",
    "\n",
    "gdf = gpd.read_parquet(PATH.joinpath('vessels_collection.parquet'))\n",
    "gaps = gdf[gdf.is_gap==True].copy()\n",
    "\n",
    "ns_gdf = gaps.cx[MIN_X:MAX_X, MIN_Y:MAX_Y]\n",
    "ns_gdf.reset_index()\n",
    "len(ns_gdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ns = ns_gdf[['geometry', 'name', 'speed_m/s']].copy()\n",
    "ns.reset_index(drop=True, inplace=True)\n",
    "\n",
    "m = ns[(ns.name=='ester') & (ns['speed_m/s'] < 10)].explore(cmap='fire', column='speed_m/s', name='Tracks', tiles='CartoDB Positron')\n",
    "# this is completely optional\n",
    "folium.LayerControl().add_to(m)\n",
    "\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vessel = ''\n",
    "\n",
    "selection = ns_gdf[(ns_gdf['name']=='atlantic_lady') & (ns_gdf['speed_m/s'] < 5)].copy()\n",
    "selection = selection[['speed_m/s', 'geometry']]\n",
    "selection.reset_index(drop=True, inplace=True)\n",
    "selection.explore(column='speed_m/s',\n",
    "                  cmap='RdYlGn',\n",
    "                  tiles='CartoDB Positron')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Loitering near infrastructure\n",
    "\n",
    "We have all the loitering events for the vessels of interest. Let's import them and see where they are loitering (according to the algorithms of Global Fishing Watch). For caveats see [this documentation](https://globalfishingwatch.org/faqs/what-is-loitering-event/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a geodataframe with all loitering events\n",
    "\n",
    "gdfs = []\n",
    "\n",
    "for file in PATH.joinpath('voi', 'gfw_data', 'events').glob('*events*.csv'):\n",
    "    filename = file.stem.split('(')[0]\n",
    "    df = pd.read_csv(file)\n",
    "    if len(df) > 0:\n",
    "        df['name'] = filename\n",
    "        gdf = gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(x=df.longitude, y=df.latitude), crs=4326)\n",
    "        gdfs.append(gdf)\n",
    "\n",
    "events = pd.concat(gdfs)\n",
    "events.to_file(PATH.joinpath('voi', 'processed', 'all_events.geojson'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or import directly\n",
    "\n",
    "events = gpd.read_file(PATH.joinpath('voi', 'processed', 'all_events.geojson'))\n",
    "len(events)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pipelines and telecom\n",
    "\n",
    "lines = gpd.read_parquet(PATH.joinpath('voi', 'context', 'gis', 'lines.parquet'))\n",
    "lines.mps_uuid = lines.mps_uuid.apply(lambda x: ''.join(random.choices(string.ascii_lowercase + string.digits, k=16)) if x is None else x)\n",
    "ns_lines = lines.cx[MIN_X:MAX_X, MIN_Y:MAX_Y].reset_index()\n",
    "len(lines)\n",
    "\n",
    "# Import platforms, connectors and seabed features\n",
    "\n",
    "points = gpd.read_parquet(PATH.joinpath('voi', 'context', 'gis', 'points.parquet'))\n",
    "points.mps_uuid = points.mps_uuid.apply(lambda x: ''.join(random.choices(string.ascii_lowercase + string.digits, k=16)) if x is None else x)\n",
    "ns_points = points.cx[MIN_X:MAX_X, MIN_Y:MAX_Y].reset_index()\n",
    "len(ns_points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform spatial joins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out non-North Sea\n",
    "gdfa = events.cx[MIN_X:MIN_Y, MAX_X:MAX_Y].copy()\n",
    "\n",
    "gdfa = gdfa.to_crs(28992)\n",
    "gdfb = ns_lines.to_crs(28992)\n",
    "\n",
    "df_n = gpd.sjoin_nearest(gdfa, gdfb).merge(gdfb, left_on=\"index_right\", right_index=True)\n",
    "\n",
    "# Calculate distance\n",
    "\n",
    "df_n[\"distance\"] = df_n.apply(lambda r: r[\"geometry_x\"].distance(r[\"geometry_y\"]) / 1000, axis=1)\n",
    "\n",
    "# and clean it up\n",
    "\n",
    "df_n.owner_x = df_n.owner_x.fillna(df_n.owner_y)\n",
    "\n",
    "df_n.rename(columns={'type': 'event_type', \n",
    "                     'name_left': 'vessel_name', \n",
    "                     'mps_uuid_x': 'mps_uuid', \n",
    "                     'owner_x': 'owner', \n",
    "                     'owner_group_x': 'owner_group',\n",
    "                     'operator_x': 'operator', \n",
    "                     'operator_group_x': 'operator_group',\n",
    "                     'name': 'infra_name', \n",
    "                     'geometry_y': 'geometry_infra',\n",
    "                     'geometry_x': 'geometry_event', \n",
    "                     'dataset_y': 'infrastructure_type'}, inplace=True)\n",
    "\n",
    "df_n.columns = df_n.columns.str.lower()\n",
    "\n",
    "df = df_n[['event_type', 'start', 'end', 'voyage', 'latitude', 'longitude', 'portvisitname', 'portvisitflag', 'vessel_name', 'geometry_event',\n",
    "           'mps_uuid', 'infrastructure_type', 'owner', 'owner_group', 'operator', 'operator_group', 'infra_name', 'geometry_infra', 'distance']].copy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find events within 5km (adapt distance to your liking)\n",
    "\n",
    "distance = 5\n",
    "df = df[df['distance'] < distance].copy()\n",
    "\n",
    "gdf = gpd.GeoDataFrame(df, geometry=df.geometry_event, crs=28992)\n",
    "gdf = gdf.drop(['geometry_infra', 'geometry_event'], axis=1)\n",
    "gdf = gdf.to_crs(4326)\n",
    "\n",
    "# Write to file\n",
    "gdf.to_file(PATH.joinpath('voi', 'processed', f'events_near_infrastructure_lines_{distance}km.geojson'), driver='GeoJSON')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geo",
   "language": "python",
   "name": "geo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
